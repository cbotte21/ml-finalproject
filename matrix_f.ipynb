{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "526b0859-a4d6-4b7e-860b-e5784f5315d2",
   "metadata": {},
   "source": [
    "Assuming we have the \"results.txt\" which contains the results of bruteforce finding the \"best\" hyperparameters (this was done through running create_model in parallel), then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f9d9fa4-68ed-45ee-9aa9-9295af922f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import vowpalwabbit\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import model_selection\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85cff76b-0075-4c93-a696-08c6c34b645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ast \n",
    "# best_rmse = np.inf\n",
    "# best_hyperparams = None\n",
    "# with open(\"results.txt\", 'r') as file:\n",
    "#     lines = file.readlines()[:107550]\n",
    "#     for line in lines:\n",
    "#         parsed_data = ast.literal_eval(line.strip())\n",
    "#         rmse = parsed_data[0]\n",
    "#         if rmse < best_rmse:\n",
    "#             best_rmse = rmse\n",
    "#             best_hyperparams = parsed_data[1]\n",
    "# print(best_rmse, best_hyperparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87d1e60-2a37-4527-87c4-5cc45ad13cfc",
   "metadata": {},
   "source": [
    "For lines 0-20,000, we have that 0.9793287079032921 {'l2': 0.0001, 'lrate': 0.01, 'passes': 1, 'rank': 39} is our best values.  \n",
    "For lines 20k-40k, we have 0.9804566075675284 {'l2': 0.0001, 'lrate': 0.01, 'passes': 6, 'rank': 39}.  \n",
    "For lines 40k-60k, we have 0.9918920165609537 {'l2': 0.03727593720314938, 'lrate': 0.01, 'passes': 6, 'rank': 15}.  \n",
    "For lines 60k-80k, we have 1.468098975181335 {'l2': 1.0, 'lrate': 0.005005, 'passes': 18, 'rank': 39}.  \n",
    "For lines 80k-100k, we have 1.0849137448164488 {'l2': 1.0, 'lrate': 0.1, 'passes': 1, 'rank': 39}.  \n",
    "For lines 100k+, 1.973363177731362 {'l2': 5.17947467923121, 'lrate': 0.1, 'passes': 18, 'rank': 39}.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c7aa81-45a3-4d04-9b91-76befe01eb03",
   "metadata": {},
   "source": [
    "So, even though these RMSE aren't the best obtainable, they were obtained using a much smaller dataset so that computation doesn't take too long (25k training, 25k validation). Even so, brute forcing through this took considerable computational time. We can see that our minimized RMSE is obtained through having l2 = 0.0001, learning rate = 0.01, passes = 1, and rank = 39, and it's noteworthy that even though we were varying our rank between 15-39, 39 usually gave the best performance, which supports the idea that allowing more space for latent features increases our performance. More testing would need to be done to determine at which point performance would begin to fall off for this particular dataset. Additionally, as the rank increases, the space the model takes up increases exponentially, so with the current computation further testing isn't feasible because of time and computational constraints. The variable \"passes\" is the number of times each training example was used during training. Now, we can train our main model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47e2a416-00ea-4064-8037-3f15699c25ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of movies: 844\n"
     ]
    }
   ],
   "source": [
    "#Credits to Cody for this part:\n",
    "# Program uses {percentageDatasetUsed} * total movies.\n",
    "percentageDatasetUsed = 0.01\n",
    "\n",
    "# Load the datasets\n",
    "folder = \"ml-32m/\"\n",
    "ratings = pd.read_csv(folder + 'ratings.csv')\n",
    "movies = pd.read_csv(folder + 'movies.csv')\n",
    "\n",
    "# Get a percentage of movie ids. Favor movies with more reviews.\n",
    "movie_rating_counts = ratings['movieId'].value_counts()\n",
    "sorted_movies = movie_rating_counts.sort_values(ascending=False)\n",
    "top_movies = sorted_movies.head(int(len(sorted_movies) * percentageDatasetUsed)).index\n",
    "top_movies_data = movies[movies[\"movieId\"].isin(top_movies)]\n",
    "print(\"Number of movies:\", len(top_movies))\n",
    "\n",
    "# Ratings of top movies, and removing timestamp feature\n",
    "filtered_ratings = ratings[ratings['movieId'].isin(top_movies)].drop(\"timestamp\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2a8d721-5d9c-4309-839b-d52635947e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import convert, split\n",
    "from model_selection import create_model, pred\n",
    "\n",
    "training_df, testing_df = split(filtered_ratings, training_size=0.8, randomstate=1) \n",
    "testing = convert(testing_df)\n",
    "training = convert(training_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a404487-aeb1-48b3-bb2d-876a80e0fe79",
   "metadata": {},
   "source": [
    "Note that running the code below creates a cache file on disk \"model.cache\" and that the \"create_model\" function takes approx. 5 minutes to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb276c1a-76e0-475d-8085-3b4ee02d850c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8811452430303447\n"
     ]
    }
   ],
   "source": [
    "hyperparams = {\"rank\": 39, \"l2\": 0.0001, \"lrate\": 0.01, \"passes\": 1} \n",
    "model, test_rmse, _ = create_model(hyperparams=hyperparams, train=training, validation_df=testing_df, validation=testing, r_model=True)\n",
    "model.save(\"model.vw\")\n",
    "print(test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a60e8b9-735a-4af3-a4a5-f834501eb8c1",
   "metadata": {},
   "source": [
    "Assuming that we have the model saved as \"model.vw\". We can begin our ranking; in practice, we'd want to compute every user against every movie and save those computations in a file, however for our purposes that isn't necessary. So, for simplicity, we'll define the ranking function just to return the top movies given some userId. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28e8942f-e755-4d8e-b1b8-14e8f2846e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vowpalwabbit.Workspace(\"-i model.vw\")\n",
    "top_n_ratings = 10\n",
    "def ranker(userId, top_n_ratings=top_n_ratings):\n",
    "    to_predict = [f\"|user {userId} |movie {movieId}\" for movieId in top_movies]\n",
    "    predictions = pred(model, to_predict)\n",
    "    top_n_movieIds, _ = zip(*sorted(list(zip(top_movies, predictions)), key=lambda x: x[1], reverse=True))\n",
    "    top_n_movies = top_movies_data[top_movies_data[\"movieId\"].isin(top_n_movieIds[:top_n_ratings])]\n",
    "    return top_n_movies[\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142c57fb-7db8-4b22-acda-19791ee7a15b",
   "metadata": {},
   "source": [
    "We just basically predict what a user would rate any given movie and return the top one; an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5dbe3198-1428-494e-a0df-64e91bef51bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257             Star Wars: Episode IV - A New Hope (1977)\n",
      "292                                   Pulp Fiction (1994)\n",
      "314                      Shawshank Redemption, The (1994)\n",
      "351                                   Forrest Gump (1994)\n",
      "475                                  Jurassic Park (1993)\n",
      "522                               Schindler's List (1993)\n",
      "585                      Silence of the Lambs, The (1991)\n",
      "2480                                   Matrix, The (1999)\n",
      "2867                                    Fight Club (1999)\n",
      "4888    Lord of the Rings: The Fellowship of the Ring,...\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "userId = 1\n",
    "top_movies = ranker(userId, top_n_ratings=top_n_ratings)\n",
    "print(top_movies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.7 (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
